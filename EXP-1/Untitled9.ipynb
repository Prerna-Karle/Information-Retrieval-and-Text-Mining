{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IgKAI9WPsiM",
        "outputId": "9e19f160-2802-4a78-e558-e9348ccba6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between Document 1 and Document 2 is:\n",
            "0.48981813079169867\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Input documents\n",
        "doc1 = \"Information retrieval deals with the representation and retrieval of information.\"\n",
        "doc2 = \"Information retrieval is the process of obtaining relevant information.\"\n",
        "\n",
        "# Store documents in a list\n",
        "documents = [doc1, doc2]\n",
        "\n",
        "# Create TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert documents into TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Compute Cosine Similarity\n",
        "similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "\n",
        "# Display result\n",
        "print(\"Cosine Similarity between Document 1 and Document 2 is:\")\n",
        "print(similarity_score[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import math\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required NLTK resources (run once)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "def cosine_similarity(doc1, doc2):\n",
        "    # Count word frequency\n",
        "    vec1 = Counter(doc1)\n",
        "    vec2 = Counter(doc2)\n",
        "\n",
        "    # Intersection of words\n",
        "    common_words = set(vec1.keys()) & set(vec2.keys())\n",
        "\n",
        "    # Dot product\n",
        "    dot_product = sum(vec1[word] * vec2[word] for word in common_words)\n",
        "\n",
        "    # Magnitude\n",
        "    magnitude1 = math.sqrt(sum(val**2 for val in vec1.values()))\n",
        "    magnitude2 = math.sqrt(sum(val**2 for val in vec2.values()))\n",
        "\n",
        "    if magnitude1 == 0 or magnitude2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return dot_product / (magnitude1 * magnitude2)\n",
        "\n",
        "# Input documents\n",
        "doc1 = \"Information retrieval is the process of obtaining information from large repositories.\"\n",
        "doc2 = \"Information retrieval deals with searching and extracting information from databases.\"\n",
        "\n",
        "# Preprocess documents\n",
        "tokens1 = preprocess(doc1)\n",
        "tokens2 = preprocess(doc2)\n",
        "\n",
        "# Compute similarity\n",
        "similarity = cosine_similarity(tokens1, tokens2)\n",
        "\n",
        "print(\"Document 1 Tokens:\", tokens1)\n",
        "print(\"Document 2 Tokens:\", tokens2)\n",
        "print(\"Cosine Similarity:\", similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URVCWaZUP3xI",
        "outputId": "4f4bf1d9-1877-4ef7-e04e-4722686c744b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1 Tokens: ['information', 'retrieval', 'process', 'obtaining', 'information', 'large', 'repositories']\n",
            "Document 2 Tokens: ['information', 'retrieval', 'deals', 'searching', 'extracting', 'information', 'databases']\n",
            "Cosine Similarity: 0.5555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4wrl-mYaTYl6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}